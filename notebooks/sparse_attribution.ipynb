{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/test_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/dictionary_learning/test_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import load_tinymodel, load_tinydataset, load_saes, load_module_names\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 533.81it/s]\n",
      "/root/dictionary_learning/notebooks/notebook_utils.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0 tokens\n",
      "Number of datapoints w/ 129 tokens: 952\n",
      "Total Tokens: 0.122808M\n"
     ]
    }
   ],
   "source": [
    "llm = load_tinymodel()\n",
    "dataset = load_tinydataset(batch_size=32, max_seq_length=128, num_datapoints=1000)\n",
    "all_saes = load_saes(k=30)\n",
    "module_names = load_module_names(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features Memory: 3.17 MB\n",
      "Virtual Weights Memory: 161.17 MB\n",
      "Attribution Memory: 20629.88 MB\n",
      "Total Memory: 20.31 GB\n"
     ]
    }
   ],
   "source": [
    "def calculate_gpu_memory(b, i, j, dtype=torch.float32):\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "    \n",
    "    input_features_memory = b * i * bytes_per_element\n",
    "    virtual_weights_memory = i * j * bytes_per_element\n",
    "    attribution_memory = b * i * j * bytes_per_element\n",
    "    \n",
    "    total_memory = input_features_memory + virtual_weights_memory + attribution_memory\n",
    "    \n",
    "    return {\n",
    "        \"input_features_memory\": input_features_memory,\n",
    "        \"virtual_weights_memory\": virtual_weights_memory,\n",
    "        \"attribution_memory\": attribution_memory,\n",
    "        \"total_memory\": total_memory,\n",
    "        \"total_memory_gb\": total_memory / (1024**3)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "b = 1*128\n",
    "i, j =  6500, 6500  # Your dimensions\n",
    "memory_info = calculate_gpu_memory(b, i, j)\n",
    "\n",
    "print(f\"Input Features Memory: {memory_info['input_features_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Virtual Weights Memory: {memory_info['virtual_weights_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Attribution Memory: {memory_info['attribution_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Total Memory: {memory_info['total_memory_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, einsum\n",
    "\n",
    "\n",
    "target_sae_names = ['torso_1_mlp_out_transcoder', 'torso_1_res_final']\n",
    "saes = [all_saes[name].to(device) for name in target_sae_names]\n",
    "resid_mid = llm.torso[1].res_mlp\n",
    "resid_final = llm.torso[1].res_final\n",
    "mlp_out = llm.torso[1].mlp\n",
    "for batch_ind, batch in enumerate(dataset):\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        with llm.trace(batch) as tracr:\n",
    "            act_res_mid = resid_mid.output.save()\n",
    "            act_res_final = resid_final.output.save()\n",
    "            act_mlp_out = mlp_out.output.save()\n",
    "        # Now we want to run through the saes\n",
    "        transcoder = saes[0].to(device)\n",
    "        sae_final = saes[1].to(device)\n",
    "        # mlp_out_hat = transcoder(act_res_mid)\n",
    "\n",
    "        # sae_final_features_hat = sae_final.encode(mlp_out_hat+act_res_mid)\n",
    "        # maybe figure out a way to fold in the decoder bias?\n",
    "        tr_dec = transcoder.decoder.weight\n",
    "        #TODO: we removed the last weight to help w/ knowing .T and shape. \n",
    "        final_enc = sae_final.encoder.weight\n",
    "        virtual_weights = tr_dec.T @ final_enc.T\n",
    "\n",
    "        act_res_mid = act_res_mid.to(device)\n",
    "        act_res_mid = rearrange(act_res_mid, 'b s d_model -> (b s) d_model')\n",
    "        input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "        # input_features = rearrange(input_features, 'b s f -> (b s) f')\n",
    "        # input_acts = rearrange(input_acts, 'b s f-> (b s) f')\n",
    "        # input_indices = rearrange(input_indices, 'b s f -> (b s) f')\n",
    "        mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "        output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "\n",
    "        # Gradient equals the weights\n",
    "        # attribution = torch.einsum('bi,ij->bij', input_features, virtual_weights)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 129, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_res_mid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False,  ..., False, False, False], device='cuda:0'),\n",
       " torch.Size([4128, 30]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_acts[output_indices==0]\n",
    "input_acts[output_indices==0]\n",
    "(output_indices==0).sum(-1) != 0, input_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2369],\n",
       "        [3193],\n",
       "        [3194],\n",
       "        [3204],\n",
       "        [3207],\n",
       "        [3798]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((output_indices==0).sum(-1) != 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_feature = 0\n",
    "num_input_features = input_features.shape[-1]\n",
    "num_output_features = output_features.shape[-1]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features)\n",
    "features_set_yet = torch.zeros(num_output_features, dtype=torch.bool)\n",
    "for current_output_feature in range(num_output_features):\n",
    "    # Get the batch indices where the output feature is non-zero\n",
    "    nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "    output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "    # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "    nz_input_ind = input_indices[nz_batch_indices]\n",
    "    batched_virtual_weights = output_virtual_weights[nz_input_ind]\n",
    "    nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "    # Calculate the attribution ie act*gradient\n",
    "    current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "    # Normalize the attributions (by abs value cause negative gradients)\n",
    "    total_abs_value = current_output_attribution.abs().sum(dim=-1)\n",
    "    normed_current_output_attribution = current_output_attribution / total_abs_value[:, None]\n",
    "\n",
    "    # Set the feature by feature attribution (average w/ existing attributions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0'),\n",
       " tensor(0.7414, device='cuda:0'))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.abs().sum(dim=-1), normed_current_output_attribution.mean(dim=0).abs().sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.abs().mean(dim=0).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 30])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Normalize: \u001b[39;00m\n\u001b[1;32m      3\u001b[0m normed_current_output_attribution \u001b[38;5;241m=\u001b[39m current_output_attribution \u001b[38;5;241m/\u001b[39m total_abs_value[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m normed_current_output_attribution\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_abs_value = current_output_attribution.abs().sum(dim=-1)\n",
    "# Normalize: \n",
    "normed_current_output_attribution = current_output_attribution / total_abs_value[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6227e-02, -2.6679e-03, -2.5494e-02, -1.2283e-03,  1.7044e-01,\n",
       "         -4.9746e-02, -1.8890e-02, -6.3689e-02, -3.3854e-02,  1.7237e-01,\n",
       "         -6.0978e-03,  2.1763e-02, -4.3416e-02,  7.5457e-02,  2.9024e-02,\n",
       "         -6.1067e-03,  5.2058e-03, -3.8862e-02, -6.2616e-03, -1.0429e-02,\n",
       "         -2.1213e-02, -3.2377e-02, -2.1631e-02,  1.1801e-02, -4.8583e-02,\n",
       "          7.9568e-03,  3.5445e-03, -9.7144e-03, -1.8767e-02, -7.1831e-03],\n",
       "        [-1.3719e-02,  2.5239e-03,  4.5559e-01,  6.0995e-03,  1.5561e-01,\n",
       "         -4.0269e-02, -1.2523e-02, -4.5647e-02,  6.8993e-03, -1.0656e-02,\n",
       "         -5.3215e-02, -1.6520e-02, -9.7398e-03,  2.4028e-03, -1.1964e-02,\n",
       "         -5.8181e-03, -1.9840e-02, -1.1012e-03, -1.8136e-02,  2.4025e-03,\n",
       "          2.2144e-04,  1.3957e-02, -2.2930e-02,  9.7942e-03, -1.6925e-02,\n",
       "         -5.8936e-03, -6.2075e-03,  1.3838e-02, -1.9537e-02,  2.1156e-05],\n",
       "        [-9.3610e-02,  2.0581e-03,  2.8441e-03, -1.7609e-02,  3.2889e-01,\n",
       "          2.9626e-03, -5.8919e-03, -9.6776e-03,  1.6282e-02, -3.5212e-02,\n",
       "         -1.0118e-01,  1.1903e-02,  2.6110e-02, -4.1736e-02,  1.5991e-02,\n",
       "         -3.8399e-03, -5.5625e-03, -3.4391e-02, -4.4615e-02, -2.1330e-02,\n",
       "          2.6017e-03,  2.2486e-04, -3.8361e-02,  4.8169e-02,  3.6406e-02,\n",
       "          1.2068e-02, -1.4367e-02, -1.3132e-02, -2.5259e-03, -1.0446e-02],\n",
       "        [-8.1073e-03,  4.2762e-02, -5.2288e-02,  4.5075e-03,  3.3414e-01,\n",
       "          2.7188e-03, -1.4417e-02, -1.8804e-02,  1.2070e-01,  4.3481e-03,\n",
       "          4.6848e-05, -4.1434e-02, -3.2482e-02, -1.1042e-02, -1.0281e-02,\n",
       "          1.1483e-03, -3.4628e-02,  2.5828e-03, -2.0375e-02,  2.2612e-02,\n",
       "         -4.7735e-03,  3.2990e-04, -2.8831e-02, -3.4438e-02,  1.6669e-04,\n",
       "         -4.7174e-02, -1.0905e-02, -8.9945e-03, -7.7301e-02, -7.6553e-03],\n",
       "        [-5.0751e-03, -4.3663e-02,  3.6039e-03,  2.9102e-01,  6.3041e-03,\n",
       "         -1.4183e-02, -6.8183e-02,  6.1243e-03, -4.5275e-02, -2.7905e-03,\n",
       "         -1.1467e-02, -7.7277e-02,  5.9998e-03,  1.1346e-02,  7.9721e-04,\n",
       "         -3.7884e-02, -1.1661e-02, -2.0265e-02,  1.7895e-02, -5.9277e-02,\n",
       "         -1.9032e-02, -7.0333e-03,  1.2288e-02,  3.3335e-02, -2.6896e-02,\n",
       "         -1.2939e-02, -1.0968e-01,  6.8002e-03, -1.8959e-02, -1.2946e-02],\n",
       "        [ 3.2406e-03,  5.2416e-01, -2.5509e-03, -2.0306e-03, -7.5386e-03,\n",
       "          1.6643e-01,  1.2463e-02,  4.3980e-03, -1.0011e-02,  1.3485e-02,\n",
       "         -2.3036e-03, -1.9625e-03, -1.6080e-02,  8.4582e-04,  9.0141e-03,\n",
       "         -1.5201e-02, -3.6115e-03,  5.4018e-03, -1.2612e-02, -3.4631e-03,\n",
       "         -3.1816e-02, -2.7081e-03, -3.3674e-02,  1.9163e-02,  5.9009e-03,\n",
       "         -1.0380e-02, -6.0363e-02,  3.3278e-03, -1.5466e-02,  4.0239e-04]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2906, 0.4918, 0.3760, 0.3835, 0.3477, 0.5571], device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4699, device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_output_attribution[0].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input_indices = torch.unique(input_indices)\n",
    "input_features[:, unique_input_indices].isnan().any()\n",
    "unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "# output_features.shape, unique_output_indices.shape\n",
    "# virtual_weights[unique_input_indices][:, unique_output_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6144, 6143]), tensor(6143, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_weights.shape, unique_output_indices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 795.99 GiB. GPU 0 has a total capacity of 15.73 GiB of which 14.37 GiB is free. Process 2007258 has 1.35 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 93.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m sparse_virtual_weights \u001b[38;5;241m=\u001b[39m virtual_weights[unique_input_indices][:, unique_output_indices]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Perform the sparse matrix multiplication\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m spar_attr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbi,ij->bij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_input_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_virtual_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# # Create a tensor to hold the full attribution\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# # Place the sparse attribution results in the correct positions in the full attribution tensor\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# full_attribution[:, unique_input_indices[:, None], unique_output_indices] = spar_attr\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 795.99 GiB. GPU 0 has a total capacity of 15.73 GiB of which 14.37 GiB is free. Process 2007258 has 1.35 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 93.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "\n",
    "def sparse_attribution(input_features, virtual_weights, input_indices, output_indices):\n",
    "    # Find unique input and output indices across the batch\n",
    "    unique_input_indices = torch.unique(input_indices)\n",
    "    unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "    # combine batch and sequence dimensions\n",
    "    input_features = rearrange(input_features, 'b s i -> (b s) i')\n",
    "\n",
    "    # Extract relevant slices of input_features and virtual_weights\n",
    "    sparse_input_features = input_features[:, unique_input_indices]\n",
    "    sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "\n",
    "    # Perform the sparse matrix multiplication\n",
    "    sparse_attribution = torch.einsum('bi,ij->bij', sparse_input_features, sparse_virtual_weights)\n",
    "\n",
    "    # Create a tensor to hold the full attribution\n",
    "    full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\n",
    "\n",
    "    # Place the sparse attribution results in the correct positions in the full attribution tensor\n",
    "    full_attribution[:, unique_input_indices[:, None], unique_output_indices] = sparse_attribution\n",
    "\n",
    "    return full_attribution\n",
    "\n",
    "# Usage\n",
    "# Assuming input_features, virtual_weights, input_indices, and output_indices are defined\n",
    "# attribution = sparse_attribution(input_features, virtual_weights, input_indices, output_indices)\n",
    "\n",
    "unique_input_indices = torch.unique(input_indices)\n",
    "unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "# Extract relevant slices of input_features and virtual_weights\n",
    "sparse_input_features = input_features[:, unique_input_indices]\n",
    "sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "\n",
    "# Perform the sparse matrix multiplication\n",
    "spar_attr = torch.einsum('bi,ij->bij', sparse_input_features, sparse_virtual_weights)\n",
    "\n",
    "# # Create a tensor to hold the full attribution\n",
    "# full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\n",
    "\n",
    "# # Place the sparse attribution results in the correct positions in the full attribution tensor\n",
    "# full_attribution[:, unique_input_indices[:, None], unique_output_indices] = spar_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 6144]), torch.Size([4292]), torch.Size([6030]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, unique_input_indices.shape, unique_output_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 4292]), torch.Size([4292, 6030]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_input_features.shape, sparse_virtual_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features shape: torch.Size([64, 129, 6144])\n",
      "unique_input_indices shape: torch.Size([4292])\n",
      "Max value in unique_input_indices: 6142\n",
      "Min value in unique_input_indices: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"input_features shape:\", input_features.shape)\n",
    "unique_input_indices = torch.unique(input_indices)\n",
    "print(\"unique_input_indices shape:\", unique_input_indices.shape)\n",
    "print(\"Max value in unique_input_indices:\", unique_input_indices.max().item())\n",
    "print(\"Min value in unique_input_indices:\", unique_input_indices.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_input_indices = input_indices.unique()\n",
    "unique_output_indices = output_indices.unique()\n",
    "\n",
    "# # Extract relevant slices of input_features and virtual_weights\n",
    "sparse_input_features = input_features[:, unique_input_indices]\n",
    "sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "# sparse_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4292,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = input_indices.unique().cpu().numpy()\n",
    "uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "input_features.index([0,1], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_attribution\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming input_features, virtual_weights, input_indices, and output_indices are defined\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# attribution = sparse_attribution(input_features, virtual_weights, input_indices, output_indices)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m unique_input_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m unique_output_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(output_indices)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Extract relevant slices of input_features and virtual_weights\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:997\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 997\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:911\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    903\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    905\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1160.81 GiB. GPU 0 has a total capacity of 15.73 GiB of which 13.95 GiB is free. Process 1969018 has 1.78 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 37.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, einsum\n\u001b[1;32m      2\u001b[0m input_features \u001b[38;5;241m=\u001b[39m rearrange(input_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s f -> (b s) f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb f1, f1 f2 -> b f1 f2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/einops/einops.py:907\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    905\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    906\u001b[0m pattern \u001b[38;5;241m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern, \u001b[38;5;241m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1160.81 GiB. GPU 0 has a total capacity of 15.73 GiB of which 13.95 GiB is free. Process 1969018 has 1.78 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 37.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "input_features = rearrange(input_features, 'b s f -> (b s) f')\n",
    "attribution = einsum(input_features, virtual_weights, \"b f1, f1 f2 -> b f1 f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# RuntimeError: self must be a matrix\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# attribution = input_features * virtual_weights\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "attribution = torch.dot(input_features, virtual_weights)\n",
    "# RuntimeError: self must be a matrix\n",
    "# attribution = input_features * virtual_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 6144]), torch.Size([6144, 6143]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, virtual_weights.shape\n",
    "attribution = torch.einsum('bi,ij->bij', input_features, virtual_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = rearrange(attribution, 'b s f -> (b s) f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 129, 6143])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indices = rearrange(output_indices, 'b s f -> (b s) f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_indices[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attribution = input_features @ virtual_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 129, 6144]), torch.Size([6144, 6143]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, virtual_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate attribution = act*gradient\n",
    "\n",
    "# I believe this is equivalent to the weights of the activations (ignore biases)\n",
    "# It'd be good to actually verify this is the case\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    tr_dec = transcoder.decoder.weight\n",
    "    #TODO: we removed the last weight to help w/ knowing .T and shape. \n",
    "    final_enc = sae_final.encoder.weight[:-1]\n",
    "    virtual_weights = tr_dec.T @ final_enc.T\n",
    "\n",
    "    act_res_mid = act_res_mid.to(device)\n",
    "    input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "    mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "    output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "\n",
    "    # For efficient gradient calculation, we can get the nonzero_indices of both input & output feature\n",
    "\n",
    "    # W_input = transcoder.decoder.weight[input_indices]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 6144]), torch.Size([64, 129, 30]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcoder.decoder.weight.shape, input_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(self, x: torch.Tensor, return_topk: bool = False):\n",
    "#     post_relu_feat_acts_BF = nn.functional.relu(self.encoder(x - self.b_dec))\n",
    "#     post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)\n",
    "\n",
    "#     # We can't split immediately due to nnsight\n",
    "#     tops_acts_BK = post_topk.values\n",
    "#     top_indices_BK = post_topk.indices\n",
    "\n",
    "#     buffer_BF = torch.zeros_like(post_relu_feat_acts_BF)\n",
    "#     encoded_acts_BF = buffer_BF.scatter_(dim=-1, index=top_indices_BK, src=tops_acts_BK)\n",
    "\n",
    "#     if return_topk:\n",
    "#         return encoded_acts_BF, tops_acts_BK, top_indices_BK\n",
    "#     else:\n",
    "#         return encoded_acts_BF\n",
    "\n",
    "# def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     return self.decoder(x) + self.b_dec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
