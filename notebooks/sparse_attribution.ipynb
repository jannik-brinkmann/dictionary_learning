{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/circuits/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/dictionary_learning/circuits/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import load_tinymodel, load_tinydataset, load_saes\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 460.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0 tokens\n",
      "Number of datapoints w/ 129 tokens: 9473\n",
      "Total Tokens: 1.222017M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/dictionary_learning/notebooks/notebook_utils.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "llm = load_tinymodel()\n",
    "dataset = load_tinydataset(batch_size=1, max_seq_length=128, num_datapoints=10000)\n",
    "all_saes = load_saes(k=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_module_names(llm):\n",
    "#     module_names = []\n",
    "#     for name, module in llm.named_modules():\n",
    "#         module_names.append(name)\n",
    "#     return module_names\n",
    "# module_names = load_module_names(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features Memory: 3.17 MB\n",
      "Virtual Weights Memory: 161.17 MB\n",
      "Attribution Memory: 20629.88 MB\n",
      "Total Memory: 20.31 GB\n"
     ]
    }
   ],
   "source": [
    "def calculate_gpu_memory(b, i, j, dtype=torch.float32):\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "    \n",
    "    input_features_memory = b * i * bytes_per_element\n",
    "    virtual_weights_memory = i * j * bytes_per_element\n",
    "    attribution_memory = b * i * j * bytes_per_element\n",
    "    \n",
    "    total_memory = input_features_memory + virtual_weights_memory + attribution_memory\n",
    "    \n",
    "    return {\n",
    "        \"input_features_memory\": input_features_memory,\n",
    "        \"virtual_weights_memory\": virtual_weights_memory,\n",
    "        \"attribution_memory\": attribution_memory,\n",
    "        \"total_memory\": total_memory,\n",
    "        \"total_memory_gb\": total_memory / (1024**3)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "b = 1*128\n",
    "i, j =  6500, 6500  # Your dimensions\n",
    "memory_info = calculate_gpu_memory(b, i, j)\n",
    "\n",
    "print(f\"Input Features Memory: {memory_info['input_features_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Virtual Weights Memory: {memory_info['virtual_weights_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Attribution Memory: {memory_info['attribution_memory'] / (1024**2):.2f} MB\")\n",
    "print(f\"Total Memory: {memory_info['total_memory_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient but only on each batch, no accumulation\n",
    "Accumulating gradients over batches != gradient over a large batch\n",
    "We also can't backpropogate through the computation graph over the accumulated normalized sum because the computation graph doesn't exist, lol. \n",
    "\n",
    "There still might be a way to calculate gradient correctly, but I'm intentionally making that out of scope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.929481029510498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def smooth_abs(x, epsilon=1e-8):\n",
    "    # add a zero clamp\n",
    "    smoothed = torch.sqrt(x**2 )\n",
    "    return torch.clamp(smoothed, min=0)\n",
    "target_sae_names = ['torso_1_mlp_out_transcoder', 'torso_1_res_final']\n",
    "saes = [all_saes[name].to(device) for name in target_sae_names]\n",
    "num_input_features = saes[0].encoder.weight.shape[0]\n",
    "num_output_features = saes[1].encoder.weight.shape[0]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features).to(device)\n",
    "running_total_for_each_feature = torch.zeros(num_output_features).to(device)\n",
    "\n",
    "resid_mid = llm.torso[1].res_mlp\n",
    "resid_final = llm.torso[1].res_final\n",
    "mlp_out = llm.torso[1].mlp\n",
    "# for batch_ind, batch in enumerate(dataset):\n",
    "# add tqdm to enumerate correctly\n",
    "#\n",
    "entropy_across_batches = []\n",
    "# Now we want to run through the saes\n",
    "transcoder = saes[0].to(device)\n",
    "sae_final = saes[1].to(device)\n",
    "\n",
    "# Virtual weights = upstream_decoder @ downstream_encoder [feature, feature]\n",
    "tr_dec = transcoder.decoder.weight\n",
    "final_enc = sae_final.encoder.weight\n",
    "virtual_weights = tr_dec.T @ final_enc.T\n",
    "# Set an optimizer on both dec and enc\n",
    "optimizer = torch.optim.Adam([transcoder.decoder.weight, transcoder.encoder.weight, sae_final.encoder.weight], lr=1e-3)\n",
    "grads_t_dec = []\n",
    "grads_t_enc = []\n",
    "grads_f_enc = []\n",
    "# check_grads_every_N = 10\n",
    "for batch_ind, batch in enumerate(tqdm(dataset)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad(), llm.trace(batch) as tracr:\n",
    "        act_res_mid = resid_mid.output.save()\n",
    "        act_res_final = resid_final.output.save()\n",
    "        act_mlp_out = mlp_out.output.save()\n",
    "\n",
    "    # Get the input & output activations and indices\n",
    "    act_res_mid = act_res_mid.to(device)\n",
    "    act_res_mid = rearrange(act_res_mid, 'b s d_model -> (b s) d_model')\n",
    "    input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "    mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "    output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "    \n",
    "    new_f_by_f = torch.zeros_like(feature_by_feature_attribution)\n",
    "    for current_output_feature in range(num_output_features):\n",
    "        # Get the batch indices where the output feature is non-zero\n",
    "        # output_indices is [batch, k], so find where k is the current output feature\n",
    "        # sum over feature/k dim, which is at most 1, since a feature can only be activated once/datapoint\n",
    "        nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "        current_output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "        # TODO: concern that I'm not adding nz_input_ind into the right slots (it's [batch, k] w/ different values for k which define different index)\n",
    "        # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "        nz_input_ind = input_indices[nz_batch_indices]\n",
    "        batched_virtual_weights = current_output_virtual_weights[nz_input_ind].to(device)\n",
    "        nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "        # Calculate the attribution ie act*gradient\n",
    "        current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "        # The new count is the number of non-zero batch indices for this feature\n",
    "        # We need this to update the average; whether or not the current output feature activates on 1 datapoint or\n",
    "        # 100 datapoints in this batch will affect the avg.\n",
    "        new_count = len(nz_batch_indices)\n",
    "\n",
    "        # Update the total attribution, we'll divide by the total count later\n",
    "        # Do scatter add to account for duplicated indices (commented out code just gets last value, not summed)\n",
    "        feature_by_feature_attribution[:, current_output_feature].scatter_add_(\n",
    "            0, \n",
    "            nz_input_ind.flatten(),\n",
    "            current_output_attribution.flatten(),\n",
    "        )\n",
    "        # feature_by_feature_attribution[nz_input_ind, current_output_feature] += current_output_attribution\n",
    "        running_total_for_each_feature[current_output_feature] += new_count\n",
    "    # Now we want to divide feature_by_feature_attribution by each of the times it activated\n",
    "    alive_output_features = running_total_for_each_feature != 0\n",
    "\n",
    "    averaged_feature_by_feature_attribution = feature_by_feature_attribution[:, alive_output_features] / running_total_for_each_feature[None, alive_output_features]\n",
    "\n",
    "    EPS = 1e-20\n",
    "    pos_avg = torch.clamp(torch.sqrt(averaged_feature_by_feature_attribution**2 + EPS) - EPS, min=0)\n",
    "    # Now we want to convert to a prob-dist and calculate entropy on it, ignoring dead features\n",
    "    normed_feature_by_feature_attribution = pos_avg / (pos_avg.sum(dim=0, keepdim=True))\n",
    "    logged = torch.log(normed_feature_by_feature_attribution)\n",
    "    entropy_loss = -(normed_feature_by_feature_attribution * logged).sum(dim=0).mean()\n",
    "    entropy_loss.backward()\n",
    "    print(f\"Entropy: {entropy_loss.item()}\")\n",
    "    \n",
    "    # get the gradients\n",
    "    grad_t_dec = transcoder.decoder.weight.grad\n",
    "    grad_t_enc = transcoder.encoder.weight.grad\n",
    "    grad_f_enc = sae_final.encoder.weight.grad\n",
    "    grads_t_dec.append(grad_t_dec.cpu())\n",
    "    grads_t_enc.append(grad_t_enc.cpu())\n",
    "    grads_f_enc.append(grad_f_enc.cpu())\n",
    "    # detect any nans\n",
    "    if torch.isnan(grad_t_dec).any() or torch.isnan(grad_t_enc).any() or torch.isnan(grad_f_enc).any():\n",
    "        print(\"nan detected\")\n",
    "        print(\"dec\")\n",
    "        print(grad_t_dec)\n",
    "        print(\"enc\")\n",
    "        print(grad_t_enc)\n",
    "        print(\"f_enc\")\n",
    "        print(grad_f_enc)\n",
    "        assert False \n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.0866947174072266\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m logged \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(normed_feature_by_feature_attribution)\n\u001b[1;32m    110\u001b[0m entropy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(normed_feature_by_feature_attribution \u001b[38;5;241m*\u001b[39m logged)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 111\u001b[0m \u001b[43mentropy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Store gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "target_sae_names = ['torso_1_mlp_out_transcoder', 'torso_1_res_final']\n",
    "saes = [all_saes[name].to(device) for name in target_sae_names]\n",
    "num_input_features = saes[0].encoder.weight.shape[0]\n",
    "num_output_features = saes[1].encoder.weight.shape[0]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features).to(device)\n",
    "running_total_for_each_feature = torch.zeros(num_output_features).to(device)\n",
    "\n",
    "resid_mid = llm.torso[1].res_mlp\n",
    "resid_final = llm.torso[1].res_final\n",
    "mlp_out = llm.torso[1].mlp\n",
    "# for batch_ind, batch in enumerate(dataset):\n",
    "# add tqdm to enumerate correctly\n",
    "#\n",
    "entropy_across_batches = []\n",
    "# Now we want to run through the saes\n",
    "transcoder = saes[0].to(device)\n",
    "sae_final = saes[1].to(device)\n",
    "\n",
    "# Virtual weights = upstream_decoder @ downstream_encoder [feature, feature]\n",
    "tr_dec = transcoder.decoder.weight\n",
    "final_enc = sae_final.encoder.weight\n",
    "# Set an optimizer on both dec and enc\n",
    "# optimizer = torch.optim.Adam([transcoder.decoder.weight, transcoder.encoder.weight, sae_final.encoder.weight], lr=1e-3)\n",
    "grads_t_dec = []\n",
    "grads_t_enc = []\n",
    "grads_f_enc = []\n",
    "current_dataset = None\n",
    "for  batch_size in range(1, 128):\n",
    "        # Manually zero gradients\n",
    "    if transcoder.decoder.weight.grad is not None:\n",
    "        transcoder.decoder.weight.grad.zero_()\n",
    "    if transcoder.encoder.weight.grad is not None:\n",
    "        transcoder.encoder.weight.grad.zero_()\n",
    "    if sae_final.encoder.weight.grad is not None:\n",
    "        sae_final.encoder.weight.grad.zero_()\n",
    "    # optimizer.zero_grad()\n",
    "    # dataset is batch-1, so just combine the first batch_size together\n",
    "    if current_dataset is None:\n",
    "        current_dataset = next(iter(dataset))\n",
    "    else:\n",
    "        current_dataset = torch.cat([current_dataset, next(iter(dataset))], dim=0)\n",
    "    batch = current_dataset\n",
    "\n",
    "\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad(), llm.trace(batch) as tracr:\n",
    "        act_res_mid = resid_mid.output.save()\n",
    "        act_res_final = resid_final.output.save()\n",
    "        act_mlp_out = mlp_out.output.save()\n",
    "\n",
    "    # Get the input & output activations and indices\n",
    "    act_res_mid = act_res_mid.detach().to(device)\n",
    "    act_res_mid = rearrange(act_res_mid, 'b s d_model -> (b s) d_model')\n",
    "\n",
    "        # Forward pass through SAEs\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # virtual_weights = tr_dec.T @ final_enc.T\n",
    "        virtual_weights = transcoder.decoder.weight.T @ sae_final.encoder.weight.T\n",
    "\n",
    "        input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "        mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "        output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "        \n",
    "        new_f_by_f = torch.zeros_like(feature_by_feature_attribution)\n",
    "        # for current_output_feature in range(num_output_features):\n",
    "        for current_output_feature in range(2):\n",
    "            # Get the batch indices where the output feature is non-zero\n",
    "            # output_indices is [batch, k], so find where k is the current output feature\n",
    "            # sum over feature/k dim, which is at most 1, since a feature can only be activated once/datapoint\n",
    "            nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "            current_output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "            # TODO: concern that I'm not adding nz_input_ind into the right slots (it's [batch, k] w/ different values for k which define different index)\n",
    "            # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "            nz_input_ind = input_indices[nz_batch_indices]\n",
    "            batched_virtual_weights = current_output_virtual_weights[nz_input_ind].to(device)\n",
    "            nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "            # Calculate the attribution ie act*gradient\n",
    "            current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "            # The new count is the number of non-zero batch indices for this feature\n",
    "            # We need this to update the average; whether or not the current output feature activates on 1 datapoint or\n",
    "            # 100 datapoints in this batch will affect the avg.\n",
    "            new_count = len(nz_batch_indices)\n",
    "\n",
    "            # Update the total attribution, we'll divide by the total count later\n",
    "            # Do scatter add to account for duplicated indices (commented out code just gets last value, not summed)\n",
    "            feature_by_feature_attribution[:, current_output_feature].scatter_add_(\n",
    "                0, \n",
    "                nz_input_ind.flatten(),\n",
    "                current_output_attribution.flatten(),\n",
    "            )\n",
    "            # feature_by_feature_attribution[nz_input_ind, current_output_feature] += current_output_attribution\n",
    "            running_total_for_each_feature[current_output_feature] += new_count\n",
    "        # Now we want to divide feature_by_feature_attribution by each of the times it activated\n",
    "        alive_output_features = running_total_for_each_feature != 0\n",
    "\n",
    "        averaged_feature_by_feature_attribution = feature_by_feature_attribution[:, alive_output_features] / running_total_for_each_feature[None, alive_output_features]\n",
    "\n",
    "        EPS = 1e-20\n",
    "        pos_avg = torch.clamp(torch.sqrt(averaged_feature_by_feature_attribution**2 + EPS) - EPS, min=0)\n",
    "        # Now we want to convert to a prob-dist and calculate entropy on it, ignoring dead features\n",
    "        normed_feature_by_feature_attribution = pos_avg / (pos_avg.sum(dim=0, keepdim=True))\n",
    "        logged = torch.log(normed_feature_by_feature_attribution)\n",
    "        entropy_loss = -(normed_feature_by_feature_attribution * logged).sum(dim=0).mean()\n",
    "        entropy_loss.backward()\n",
    "        print(f\"Entropy: {entropy_loss.item()}\")\n",
    "        \n",
    "        # Store gradients\n",
    "        grads_t_dec.append(transcoder.decoder.weight.grad.clone().cpu())\n",
    "        grads_t_enc.append(transcoder.encoder.weight.grad.clone().cpu())\n",
    "        grads_f_enc.append(sae_final.encoder.weight.grad.clone().cpu())\n",
    "        \n",
    "        # Optional: check for NaNs\n",
    "        if any(torch.isnan(grad).any() for grad in [grads_t_dec[-1], grads_t_enc[-1], grads_f_enc[-1]]):\n",
    "            print(\"NaN detected in gradients\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset is type DataLoader. Get the first 2 batches\n",
    "batch = next(iter(dataset))\n",
    "new_batch = torch.cat([batch, next(iter(dataset))], dim=0)\n",
    "# now make this a loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5772, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_feature_by_feature_attribution.abs().sum(dim=0).count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_avg.sum(dim=0, keepdim=True) == 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just calculate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_saes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# def update_running_average_with_sum(current_avg, new_sum, current_count, new_count):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     Update the running average with a new sum and count.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     updated_avg = updated_sum / total_count\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     return updated_avg, total_count\u001b[39;00m\n\u001b[1;32m     21\u001b[0m target_sae_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorso_1_mlp_out_transcoder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorso_1_res_final\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m saes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mall_saes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_sae_names\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m num_input_features \u001b[38;5;241m=\u001b[39m saes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m num_output_features \u001b[38;5;241m=\u001b[39m saes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# def update_running_average_with_sum(current_avg, new_sum, current_count, new_count):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     Update the running average with a new sum and count.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     updated_avg = updated_sum / total_count\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     return updated_avg, total_count\u001b[39;00m\n\u001b[1;32m     21\u001b[0m target_sae_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorso_1_mlp_out_transcoder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorso_1_res_final\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m saes \u001b[38;5;241m=\u001b[39m [\u001b[43mall_saes\u001b[49m[name]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m target_sae_names]\n\u001b[1;32m     23\u001b[0m num_input_features \u001b[38;5;241m=\u001b[39m saes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m num_output_features \u001b[38;5;241m=\u001b[39m saes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_saes' is not defined"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "# def update_running_average_with_sum(current_avg, new_sum, current_count, new_count):\n",
    "#     \"\"\"\n",
    "#     Update the running average with a new sum and count.\n",
    "    \n",
    "#     :param current_avg: Current running average\n",
    "#     :param new_sum: Sum of new values to include in the average\n",
    "#     :param current_count: Number of values in the current average\n",
    "#     :param new_count: Number of new values\n",
    "#     :return: New running average, Updated total count\n",
    "#     \"\"\"\n",
    "#     if current_count == 0:\n",
    "#         return new_sum / new_count, new_count\n",
    "    \n",
    "#     total_count = current_count + new_count\n",
    "#     updated_sum = current_avg * current_count + new_sum\n",
    "#     updated_avg = updated_sum / total_count\n",
    "#     return updated_avg, total_count\n",
    "target_sae_names = ['torso_1_mlp_out_transcoder', 'torso_1_res_final']\n",
    "saes = [all_saes[name].to(device) for name in target_sae_names]\n",
    "num_input_features = saes[0].encoder.weight.shape[0]\n",
    "num_output_features = saes[1].encoder.weight.shape[0]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features).to(device)\n",
    "running_total_for_each_feature = torch.zeros(num_output_features).to(device)\n",
    "\n",
    "resid_mid = llm.torso[1].res_mlp\n",
    "resid_final = llm.torso[1].res_final\n",
    "mlp_out = llm.torso[1].mlp\n",
    "# for batch_ind, batch in enumerate(dataset):\n",
    "# add tqdm to enumerate correctly\n",
    "#\n",
    "entropy_across_batches = []\n",
    "# Now we want to run through the saes\n",
    "transcoder = saes[0].to(device)\n",
    "sae_final = saes[1].to(device)\n",
    "\n",
    "# Virtual weights = upstream_decoder @ downstream_encoder [feature, feature]\n",
    "tr_dec = transcoder.decoder.weight\n",
    "final_enc = sae_final.encoder.weight\n",
    "virtual_weights = tr_dec.T @ final_enc.T\n",
    "# Set an optimizer on both dec and enc\n",
    "grads_t_dec = []\n",
    "grads_t_enc = []\n",
    "grads_f_enc = []\n",
    "check_grads_every_N = 1\n",
    "with torch.no_grad():\n",
    "    for batch_ind, batch in enumerate(tqdm(dataset)):\n",
    "    \n",
    "\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad(), llm.trace(batch) as tracr:\n",
    "            act_res_mid = resid_mid.output.save()\n",
    "            act_res_final = resid_final.output.save()\n",
    "            act_mlp_out = mlp_out.output.save()\n",
    "\n",
    "        # Get the input & output activations and indices\n",
    "        act_res_mid = act_res_mid.to(device)\n",
    "        act_res_mid = rearrange(act_res_mid, 'b s d_model -> (b s) d_model')\n",
    "        input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "        mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "        output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "        \n",
    "        new_f_by_f = torch.zeros_like(feature_by_feature_attribution)\n",
    "        for current_output_feature in range(num_output_features):\n",
    "            # Get the batch indices where the output feature is non-zero\n",
    "            # output_indices is [batch, k], so find where k is the current output feature\n",
    "            # sum over feature/k dim, which is at most 1, since a feature can only be activated once/datapoint\n",
    "            nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "            current_output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "            # TODO: concern that I'm not adding nz_input_ind into the right slots (it's [batch, k] w/ different values for k which define different index)\n",
    "            # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "            nz_input_ind = input_indices[nz_batch_indices]\n",
    "            batched_virtual_weights = current_output_virtual_weights[nz_input_ind].to(device)\n",
    "            nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "            # Calculate the attribution ie act*gradient\n",
    "            current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "            # The new count is the number of non-zero batch indices for this feature\n",
    "            # We need this to update the average; whether or not the current output feature activates on 1 datapoint or\n",
    "            # 100 datapoints in this batch will affect the avg.\n",
    "            new_count = len(nz_batch_indices)\n",
    "\n",
    "            # Update the total attribution, we'll divide by the total count later\n",
    "            # Do scatter add to account for duplicated indices (commented out code just gets last value, not summed)\n",
    "            feature_by_feature_attribution[:, current_output_feature].scatter_add_(\n",
    "                0, \n",
    "                nz_input_ind.flatten(),\n",
    "                current_output_attribution.flatten(),\n",
    "            )\n",
    "            # feature_by_feature_attribution[nz_input_ind, current_output_feature] += current_output_attribution\n",
    "            running_total_for_each_feature[current_output_feature] += new_count\n",
    "        if(batch_ind % check_grads_every_N == 0):\n",
    "            # Now we want to divide feature_by_feature_attribution by each of the times it activated\n",
    "            alive_output_features = running_total_for_each_feature != 0\n",
    "\n",
    "            averaged_feature_by_feature_attribution = feature_by_feature_attribution[:, alive_output_features] / running_total_for_each_feature[None, alive_output_features]\n",
    "            # Now we want to convert to a prob-dist and calculate entropy on it, ignoring dead features\n",
    "            normed_feature_by_feature_attribution = averaged_feature_by_feature_attribution / averaged_feature_by_feature_attribution.abs().sum(dim=0)\n",
    "\n",
    "            logged = normed_feature_by_feature_attribution.abs().log()\n",
    "            logged[logged.isinf()] = 0\n",
    "            entropy = -(normed_feature_by_feature_attribution.abs() * logged).sum(dim=0)\n",
    "            entropy_loss = entropy.mean()\n",
    "            entropy_across_batches.append(entropy_loss.item())\n",
    "            print(f\"Entropy: {entropy_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Sparse Attribution but w/ Gradients\n",
    "We need to make it differentiable (ie abs to smooth abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.929481029510498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, 0.],\n",
      "        [nan, nan, nan,  ..., nan, nan, 0.],\n",
      "        [nan, nan, nan,  ..., nan, nan, 0.],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, 0.],\n",
      "        [nan, nan, nan,  ..., nan, nan, 0.],\n",
      "        [nan, nan, nan,  ..., nan, nan, 0.]], device='cuda:0')\n",
      "enc\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "f_enc\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "# def update_running_average_with_sum(current_avg, new_sum, current_count, new_count):\n",
    "#     \"\"\"\n",
    "#     Update the running average with a new sum and count.\n",
    "    \n",
    "#     :param current_avg: Current running average\n",
    "#     :param new_sum: Sum of new values to include in the average\n",
    "#     :param current_count: Number of values in the current average\n",
    "#     :param new_count: Number of new values\n",
    "#     :return: New running average, Updated total count\n",
    "#     \"\"\"\n",
    "#     if current_count == 0:\n",
    "#         return new_sum / new_count, new_count\n",
    "    \n",
    "#     total_count = current_count + new_count\n",
    "#     updated_sum = current_avg * current_count + new_sum\n",
    "#     updated_avg = updated_sum / total_count\n",
    "#     return updated_avg, total_count\n",
    "def smooth_abs(x, epsilon=1e-8):\n",
    "    # add a zero clamp\n",
    "    smoothed = torch.sqrt(x**2 )\n",
    "    return torch.clamp(smoothed, min=0)\n",
    "target_sae_names = ['torso_1_mlp_out_transcoder', 'torso_1_res_final']\n",
    "saes = [all_saes[name].to(device) for name in target_sae_names]\n",
    "num_input_features = saes[0].encoder.weight.shape[0]\n",
    "num_output_features = saes[1].encoder.weight.shape[0]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features).to(device)\n",
    "running_total_for_each_feature = torch.zeros(num_output_features).to(device)\n",
    "\n",
    "resid_mid = llm.torso[1].res_mlp\n",
    "resid_final = llm.torso[1].res_final\n",
    "mlp_out = llm.torso[1].mlp\n",
    "# for batch_ind, batch in enumerate(dataset):\n",
    "# add tqdm to enumerate correctly\n",
    "#\n",
    "entropy_across_batches = []\n",
    "# Now we want to run through the saes\n",
    "transcoder = saes[0].to(device)\n",
    "sae_final = saes[1].to(device)\n",
    "\n",
    "# Virtual weights = upstream_decoder @ downstream_encoder [feature, feature]\n",
    "tr_dec = transcoder.decoder.weight\n",
    "final_enc = sae_final.encoder.weight\n",
    "virtual_weights = tr_dec.T @ final_enc.T\n",
    "# Set an optimizer on both dec and enc\n",
    "optimizer = torch.optim.Adam([transcoder.decoder.weight, transcoder.encoder.weight, sae_final.encoder.weight], lr=1e-3)\n",
    "grads_t_dec = []\n",
    "grads_t_enc = []\n",
    "grads_f_enc = []\n",
    "# check_grads_every_N = 10\n",
    "check_grads_every_N = 1\n",
    "for batch_ind, batch in enumerate(tqdm(dataset)):\n",
    "    # if check_grads_every_N, then do with torch.no_grad()\n",
    "    # else, do with torch.enable_grad()\n",
    "    compute_grads = (batch_ind % check_grads_every_N == 0)\n",
    "    \n",
    "    # Use a single with statement\n",
    "    with torch.set_grad_enabled(compute_grads):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad(), llm.trace(batch) as tracr:\n",
    "            act_res_mid = resid_mid.output.save()\n",
    "            act_res_final = resid_final.output.save()\n",
    "            act_mlp_out = mlp_out.output.save()\n",
    "\n",
    "        # Get the input & output activations and indices\n",
    "        act_res_mid = act_res_mid.to(device)\n",
    "        act_res_mid = rearrange(act_res_mid, 'b s d_model -> (b s) d_model')\n",
    "        input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "        mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "        output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "        \n",
    "        new_f_by_f = torch.zeros_like(feature_by_feature_attribution)\n",
    "        for current_output_feature in range(num_output_features):\n",
    "            # Get the batch indices where the output feature is non-zero\n",
    "            # output_indices is [batch, k], so find where k is the current output feature\n",
    "            # sum over feature/k dim, which is at most 1, since a feature can only be activated once/datapoint\n",
    "            nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "            current_output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "            # TODO: concern that I'm not adding nz_input_ind into the right slots (it's [batch, k] w/ different values for k which define different index)\n",
    "            # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "            nz_input_ind = input_indices[nz_batch_indices]\n",
    "            batched_virtual_weights = current_output_virtual_weights[nz_input_ind].to(device)\n",
    "            nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "            # Calculate the attribution ie act*gradient\n",
    "            current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "            # The new count is the number of non-zero batch indices for this feature\n",
    "            # We need this to update the average; whether or not the current output feature activates on 1 datapoint or\n",
    "            # 100 datapoints in this batch will affect the avg.\n",
    "            new_count = len(nz_batch_indices)\n",
    "\n",
    "            # Update the total attribution, we'll divide by the total count later\n",
    "            # Do scatter add to account for duplicated indices (commented out code just gets last value, not summed)\n",
    "            feature_by_feature_attribution[:, current_output_feature].scatter_add_(\n",
    "                0, \n",
    "                nz_input_ind.flatten(),\n",
    "                current_output_attribution.flatten(),\n",
    "            )\n",
    "            # feature_by_feature_attribution[nz_input_ind, current_output_feature] += current_output_attribution\n",
    "            running_total_for_each_feature[current_output_feature] += new_count\n",
    "        if(batch_ind % check_grads_every_N == 0):\n",
    "            # Now we want to divide feature_by_feature_attribution by each of the times it activated\n",
    "            alive_output_features = running_total_for_each_feature != 0\n",
    "\n",
    "            averaged_feature_by_feature_attribution = feature_by_feature_attribution[:, alive_output_features] / running_total_for_each_feature[None, alive_output_features]\n",
    "            # Now we want to convert to a prob-dist and calculate entropy on it, ignoring dead features\n",
    "            not_smooth_normed_feature_by_feature_attribution = averaged_feature_by_feature_attribution / averaged_feature_by_feature_attribution.abs().sum(dim=0)\n",
    "            normed_feature_by_feature_attribution = averaged_feature_by_feature_attribution / smooth_abs(averaged_feature_by_feature_attribution).sum(dim=0)\n",
    "            \n",
    "            # normed_feature_by_feature_attribution = torch.nn.functional.softmax(averaged_feature_by_feature_attribution, dim=0)\n",
    "\n",
    "\n",
    "            logged = smooth_abs(normed_feature_by_feature_attribution).log()\n",
    "            not_smooth__logged = not_smooth_normed_feature_by_feature_attribution.abs().log()\n",
    "            logged[logged.isinf()] = 0\n",
    "            not_smooth__logged[not_smooth__logged.isinf()] = 0\n",
    "            entropy = -(smooth_abs(normed_feature_by_feature_attribution) * logged).sum(dim=0)\n",
    "            not_smooth_entropy = -(not_smooth_normed_feature_by_feature_attribution.abs() * not_smooth__logged).sum(dim=0)\n",
    "            # entropy = -(normed_feature_by_feature_attribution.abs() * logged).sum(dim=0)\n",
    "            entropy_loss = entropy.mean()\n",
    "            entropy_across_batches.append(entropy_loss.item())\n",
    "            print(f\"Entropy: {entropy_loss.item()}\")\n",
    "\n",
    "        #     # Get the gradient of the entropy loss\n",
    "            entropy_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_([transcoder.decoder.weight, transcoder.encoder.weight, sae_final.encoder.weight], max_norm=1.0)\n",
    "\n",
    "            # get the gradients\n",
    "            grad_t_dec = transcoder.decoder.weight.grad\n",
    "            grad_t_enc = transcoder.encoder.weight.grad\n",
    "            grad_f_enc = sae_final.encoder.weight.grad\n",
    "            grads_t_dec.append(grad_t_dec.cpu())\n",
    "            grads_t_enc.append(grad_t_enc.cpu())\n",
    "            grads_f_enc.append(grad_f_enc.cpu())\n",
    "            print(\"dec\")\n",
    "            print(grad_t_dec)\n",
    "            print(\"enc\")\n",
    "            print(grad_t_enc)\n",
    "            print(\"f_enc\")\n",
    "            print(grad_f_enc)\n",
    "        break\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: tensor([1., 0., 0., 0.])\n",
      "tensor([1., 0., 0., 0.])\n",
      "tensor([  0.0000, -23.0259, -23.0259, -23.0259])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.2100\n",
      "  Cauchy Entropy: 0.6931\n",
      "  Softplus Entropy: 3.3927\n",
      "  Gaussian Entropy: -3.6065\n",
      "  sqrt_entropy: -0.0000\n",
      "  Sparsity Inducing Loss: 0.0000\n",
      "\n",
      "Distribution: tensor([-1.,  0.,  0.,  0.])\n",
      "tensor([1., 0., 0., 0.])\n",
      "tensor([  0.0000, -23.0259, -23.0259, -23.0259])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.2100\n",
      "  Cauchy Entropy: 0.6931\n",
      "  Softplus Entropy: 2.3927\n",
      "  Gaussian Entropy: -3.6065\n",
      "  sqrt_entropy: -0.0000\n",
      "  Sparsity Inducing Loss: 0.0000\n",
      "\n",
      "Distribution: tensor([0.9900, 0.0100, 0.0000, 0.0000])\n",
      "tensor([0.9900, 0.0100, 0.0000, 0.0000])\n",
      "tensor([-1.0050e-02, -4.6052e+00, -2.3026e+01, -2.3026e+01])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.2100\n",
      "  Cauchy Entropy: 0.6932\n",
      "  Softplus Entropy: 3.3977\n",
      "  Gaussian Entropy: -3.6065\n",
      "  sqrt_entropy: 0.0560\n",
      "  Sparsity Inducing Loss: 0.0101\n",
      "\n",
      "Distribution: tensor([ 0.5000, -0.5000,  0.0000,  0.0000])\n",
      "tensor([0.5000, 0.5000, 0.0000, 0.0000])\n",
      "tensor([ -0.6931,  -0.6931, -23.0259, -23.0259])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.1293\n",
      "  Cauchy Entropy: 0.8109\n",
      "  Softplus Entropy: 2.8951\n",
      "  Gaussian Entropy: -3.5576\n",
      "  sqrt_entropy: 0.6931\n",
      "  Sparsity Inducing Loss: 0.6932\n",
      "\n",
      "Distribution: tensor([ 0.5000, -0.3000,  0.1000, -0.1000])\n",
      "tensor([0.5000, 0.3000, 0.1000, 0.1000])\n",
      "tensor([-0.6931, -1.2040, -2.3026, -2.3026])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.1332\n",
      "  Cauchy Entropy: 0.8053\n",
      "  Softplus Entropy: 3.0615\n",
      "  Gaussian Entropy: -3.5616\n",
      "  sqrt_entropy: 1.1683\n",
      "  Sparsity Inducing Loss: 0.6931\n",
      "\n",
      "Distribution: tensor([ 0.5000, -0.1667,  0.1667, -0.1667])\n",
      "tensor([0.5000, 0.1667, 0.1667, 0.1667])\n",
      "tensor([-0.6932, -1.7917, -1.7917, -1.7917])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.1371\n",
      "  Cauchy Entropy: 0.7997\n",
      "  Softplus Entropy: 3.1834\n",
      "  Gaussian Entropy: -3.5649\n",
      "  sqrt_entropy: 1.2425\n",
      "  Sparsity Inducing Loss: 0.6932\n",
      "\n",
      "Distribution: tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "tensor([-1.3863, -1.3863, -1.3863, -1.3863])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.0729\n",
      "  Cauchy Entropy: 0.8926\n",
      "  Softplus Entropy: 3.8963\n",
      "  Gaussian Entropy: -3.5300\n",
      "  sqrt_entropy: 1.3863\n",
      "  Sparsity Inducing Loss: 1.3863\n",
      "\n",
      "Distribution: tensor([4., 4., 4., 4.])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "tensor([-1.3863, -1.3863, -1.3863, -1.3863])\n",
      "  Squared Error Entropy: 1.0000\n",
      "  Tanh Entropy: 0.0729\n",
      "  Cauchy Entropy: 0.8926\n",
      "  Softplus Entropy: 3.8963\n",
      "  Gaussian Entropy: -3.5300\n",
      "  sqrt_entropy: 1.3863\n",
      "  Sparsity Inducing Loss: 1.3863\n",
      "\n",
      "Squared Error Entropy gradients: tensor([-3.5763e-07,  3.5763e-07, -5.9605e-08,  5.9605e-08])\n",
      "Tanh Entropy gradients: tensor([ 0.0156, -0.0156, -0.0778,  0.0778])\n",
      "Cauchy Entropy gradients: tensor([-0.0226,  0.0226,  0.1131, -0.1131])\n",
      "Softplus Entropy gradients: tensor([0.6930, 0.6937, 0.6951, 0.6917])\n",
      "Gaussian Entropy gradients: tensor([-0.0075,  0.0075,  0.0377, -0.0377])\n",
      "tensor([0.4167, 0.4167, 0.0833, 0.0833], grad_fn=<DivBackward0>)\n",
      "tensor([-0.8755, -0.8755, -2.4849, -2.4849], grad_fn=<LogBackward0>)\n",
      "sqrt_entropy gradients: tensor([-0.2235,  0.2235,  1.1177, -1.1177])\n",
      "Sparsity Inducing Loss gradients: tensor([-0.1667,  0.1667,  0.8333, -0.8333])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def squared_error_entropy(x):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return (normalized**2).sum(dim=0)\n",
    "\n",
    "def tanh_entropy(x, scale=1.0):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return -((torch.tanh(scale * normalized)**2).sum(dim=0) - 1) / 2\n",
    "\n",
    "def cauchy_entropy(x, scale=1.0):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return torch.log(1 + scale * normalized**2).sum(dim=0)\n",
    "\n",
    "def softplus_entropy(x, beta=1.0):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return F.softplus(beta * normalized, beta=beta).sum(dim=0) / beta\n",
    "\n",
    "def gaussian_entropy(x, sigma=1.0):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return -torch.exp(-normalized**2 / (2 * sigma**2)).sum(dim=0)\n",
    "\n",
    "def gaussian_entropy(x, sigma=1.0):\n",
    "    normalized = x / torch.sqrt((x**2).sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return -torch.exp(-normalized**2 / (2 * sigma**2)).sum(dim=0)\n",
    "\n",
    "def smooth_absolute(x):\n",
    "    return torch.sqrt(x**2 + 1e-8)\n",
    "    \n",
    "\n",
    "def sqrt_entropy(x):\n",
    "    pos_x = torch.sqrt(x**2)\n",
    "    normalized = pos_x / (pos_x.sum(dim=0, keepdim=True) + 1e-8)\n",
    "    # normalized = x / x.abs().sum(dim=0, keepdim=True)\n",
    "    # logged = normalized.abs().log()\n",
    "    logged = torch.log(normalized + 1e-10)\n",
    "    print(normalized)\n",
    "    print(logged)\n",
    "    # logged[logged.isinf()] = 0\n",
    "    # return torch.sqrt(normalized.abs()).sum(dim=0)\n",
    "    return -(normalized * logged).sum(dim=0)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ... (keep your other entropy functions)\n",
    "\n",
    "def sparsity_inducing_loss(x):\n",
    "    pos_x = torch.sqrt(x**2 + 1e-10)\n",
    "    # pos_x = torch.sqrt(x**2)\n",
    "    normalized = pos_x / (pos_x.sum(dim=0, keepdim=True) + 1e-10)\n",
    "    # normalized = pos_x / (pos_x.sum(dim=0, keepdim=True))\n",
    "    # return -torch.log(normalized.max() + 1e-10) + torch.sum(normalized) -1\n",
    "    # return -torch.log(normalized.max() + 1e-10)\n",
    "    return -torch.log(normalized.max() + 1e-10)\n",
    "    # return -torch.log(normalized.max()) + torch.sum(normalized) -1\n",
    "\n",
    "def compare_metrics(x):\n",
    "    metrics = {\n",
    "        'Squared Error Entropy': squared_error_entropy(x),\n",
    "        'Tanh Entropy': tanh_entropy(x),\n",
    "        'Cauchy Entropy': cauchy_entropy(x),\n",
    "        'Softplus Entropy': softplus_entropy(x),\n",
    "        'Gaussian Entropy': gaussian_entropy(x),\n",
    "        'sqrt_entropy': sqrt_entropy(x),\n",
    "        'Sparsity Inducing Loss': sparsity_inducing_loss(x),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_distributions = [\n",
    "        # torch.tensor([0.0, 0.0, 0.0, 0.0]),\n",
    "        # torch.tensor([5.0, 0.0, 0.0, 0.0]),\n",
    "        # torch.tensor([5.0, 0.5, 0.0, 0.0]),\n",
    "        # torch.tensor([1.0, 0.1, 0.0, 0.0]),\n",
    "        torch.tensor([1.0, 0.0, 0.0, 0.0]),\n",
    "        torch.tensor([-1.0, 0.0, 0.0, 0.0]),\n",
    "        torch.tensor([0.99, 0.01, 0.0, 0.0]),\n",
    "        torch.tensor([0.5, -0.5, 0.0, 0.0]),\n",
    "        torch.tensor([0.5, -0.3, 0.1, -0.1]),\n",
    "        torch.tensor([0.5, -0.16667, 0.16667, -0.16667]),\n",
    "        torch.tensor([0.25, 0.25, 0.25, 0.25]),\n",
    "        torch.tensor([4.0, 4.0, 4.0, 4.0]),\n",
    "    ]\n",
    "\n",
    "    for dist in test_distributions:\n",
    "        print(f\"Distribution: {dist}\")\n",
    "        for name, value in compare_metrics(dist).items():\n",
    "            print(f\"  {name}: {value.item():.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Gradient check\n",
    "    x = torch.tensor([0.5, -0.5, 0.1, -0.1], requires_grad=True)\n",
    "    for name, func in [\n",
    "        ('Squared Error Entropy', squared_error_entropy),\n",
    "        ('Tanh Entropy', tanh_entropy),\n",
    "        ('Cauchy Entropy', cauchy_entropy),\n",
    "        ('Softplus Entropy', softplus_entropy),\n",
    "        ('Gaussian Entropy', gaussian_entropy),\n",
    "        ('sqrt_entropy', sqrt_entropy),\n",
    "        ('Sparsity Inducing Loss', sparsity_inducing_loss),\n",
    "    ]:\n",
    "        y = func(x)\n",
    "        try:\n",
    "            y.backward()\n",
    "            if torch.isnan(x.grad).any():\n",
    "                print(f\"{name} | NaN in gradients\")\n",
    "            print(f\"{name} gradients: {x.grad}\")\n",
    "            x.grad.zero_()\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: Gradient computation failed - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5887, device='cuda:0')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 6\n",
    "(not_smooth_normed_feature_by_feature_attribution[:, N] == normed_feature_by_feature_attribution[:, N]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nz_ind = not_smooth_normed_feature_by_feature_attribution[:, N].nonzero()\n",
    "not_smooth_normed_feature_by_feature_attribution[nz_ind, N], normed_feature_by_feature_attribution[nz_ind, N]\n",
    "# Plot both, one as x and one as y\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(not_smooth_normed_feature_by_feature_attribution[nz_ind, N].detach().cpu(), normed_feature_by_feature_attribution[nz_ind, N].detach().cpu())\n",
    "# Draw a diagonal thin linear between the min and max of both\n",
    "min_val = not_smooth_normed_feature_by_feature_attribution[nz_ind, N].min().item()\n",
    "max_val = not_smooth_normed_feature_by_feature_attribution[nz_ind, N].max().item()\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red')\n",
    "plt.xlabel(\"Not Smoothed\")\n",
    "plt.ylabel(\"Smoothed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.7974, 1.0676, 2.3563,  ..., 2.2825, 0.9650, 0.0000], device='cuda:0',\n",
       "        grad_fn=<SumBackward1>),\n",
       " torch.Size([6144, 6107]),\n",
       " tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed = averaged_feature_by_feature_attribution / averaged_feature_by_feature_attribution.abs().sum(dim=0)\n",
    "normed.abs().sum(dim=1), normed.shape, normed.abs()[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mentropy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_([transcoder\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mweight, transcoder\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight, sae_final\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight], max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      4\u001b[0m transcoder\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/circuits/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "entropy_loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_([transcoder.decoder.weight, transcoder.encoder.weight, sae_final.encoder.weight], max_norm=1.0)\n",
    "transcoder.decoder.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9295, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot entropy_across_batches\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(entropy_across_batches)\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "# Set x indicies to be in increments of 10 to make it easier to read\n",
    "# Each tick is 128*32 tokens\n",
    "# Measure as M of tokens\n",
    "n_ticks = 50\n",
    "plt.xticks(range(0, len(entropy_across_batches), n_ticks), [f\"{i*128*32/1e6:.2f}M\" for i in range(0, len(entropy_across_batches), n_ticks)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.206844329833984]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_across_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2069, device='cuda:0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to divide feature_by_feature_attribution by each of the times it activated\n",
    "alive_output_features = running_total_for_each_feature != 0\n",
    "\n",
    "averaged_feature_by_feature_attribution = feature_by_feature_attribution[:, alive_output_features] / running_total_for_each_feature[alive_output_features].unsqueeze(0)\n",
    "# Now we want to convert to a prob-dist and calculate entropy on it, ignoring dead features\n",
    "normed_feature_by_feature_attribution = averaged_feature_by_feature_attribution / averaged_feature_by_feature_attribution.abs().sum(dim=0)\n",
    "\n",
    "logged = normed_feature_by_feature_attribution.abs().log()\n",
    "logged[logged.isinf()] = 0\n",
    "entropy = -(normed_feature_by_feature_attribution.abs() * logged).sum(dim=0)\n",
    "entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(False, device='cuda:0'),\n",
       " tensor(False, device='cuda:0'),\n",
       " tensor(False, device='cuda:0'),\n",
       " tensor(True, device='cuda:0'),\n",
       " tensor(True, device='cuda:0'),\n",
       " tensor(False, device='cuda:0'),\n",
       " tensor(True, device='cuda:0'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(running_total_for_each_feature[alive_output_features] == 0).any(), running_total_for_each_feature[alive_output_features].isnan().any(), feature_by_feature_attribution.isnan().any(), normed_feature_by_feature_attribution.isnan().any(), entropy.isnan().any(), averaged_feature_by_feature_attribution.isnan().any(), (averaged_feature_by_feature_attribution == 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged[logged.isnan()], logged.isnan().nonzero()\n",
    "normed_feature_by_feature_attribution.abs()[0, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    -inf,     -inf,     -inf,  ..., -10.5742,     -inf,     -inf],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in range(10):\n",
    "    assert (normed_feature_by_feature_attribution[:, N] == feature_by_feature_attribution[:, N] / running_total_for_each_feature[N]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False,  ..., False, False, False], device='cuda:0'),\n",
       " torch.Size([4128, 30]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_acts[output_indices==0]\n",
    "input_acts[output_indices==0]\n",
    "(output_indices==0).sum(-1) != 0, input_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2369],\n",
       "        [3193],\n",
       "        [3194],\n",
       "        [3204],\n",
       "        [3207],\n",
       "        [3798]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((output_indices==0).sum(-1) != 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_feature = 0\n",
    "num_input_features = input_features.shape[-1]\n",
    "num_output_features = output_features.shape[-1]\n",
    "feature_by_feature_attribution = torch.zeros(num_input_features, num_output_features).to(device)\n",
    "# features_set_yet = torch.zeros(num_output_features, dtype=torch.bool)\n",
    "iteration = 1\n",
    "\n",
    "for current_output_feature in range(num_output_features):\n",
    "    # Get the batch indices where the output feature is non-zero\n",
    "    nz_batch_indices = (output_indices==current_output_feature).sum(-1).nonzero()[:, 0]\n",
    "    output_virtual_weights = virtual_weights[:, current_output_feature]\n",
    "\n",
    "    # Index into the virtual weights & input indices ie find the inputs that activated the output feature\n",
    "    nz_input_ind = input_indices[nz_batch_indices]\n",
    "    batched_virtual_weights = output_virtual_weights[nz_input_ind].to(device)\n",
    "    nz_input_acts = input_acts[nz_batch_indices]\n",
    "\n",
    "    # Calculate the attribution ie act*gradient\n",
    "    current_output_attribution = nz_input_acts * batched_virtual_weights \n",
    "\n",
    "    # Set the feature by feature attribution (average w/ existing attributions)    \n",
    "    averaged_current_output_attribution = current_output_attribution.mean(dim=0)\n",
    "    tmp_feature_list = torch.zeros(num_input_features).to(device)\n",
    "    # Assign the averaged attributions to the correct input features\n",
    "    # tmp_feature_list[nz_input_ind] = averaged_current_output_attribution\n",
    "\n",
    "    feature_by_feature_attribution[nz_input_ind, current_output_feature] += averaged_current_output_attribution\n",
    "\n",
    "# Normalize the attributions (by abs value cause negative gradients)\n",
    "# total_abs_value = current_output_attribution.abs().sum(dim=-1)\n",
    "# normed_current_output_attribution = current_output_attribution / total_abs_value[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0'),\n",
       " tensor(0.7414, device='cuda:0'))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.abs().sum(dim=-1), normed_current_output_attribution.mean(dim=0).abs().sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.abs().mean(dim=0).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 30])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Normalize: \u001b[39;00m\n\u001b[1;32m      3\u001b[0m normed_current_output_attribution \u001b[38;5;241m=\u001b[39m current_output_attribution \u001b[38;5;241m/\u001b[39m total_abs_value[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m normed_current_output_attribution\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_abs_value = current_output_attribution.abs().sum(dim=-1)\n",
    "# Normalize: \n",
    "normed_current_output_attribution = current_output_attribution / total_abs_value[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6227e-02, -2.6679e-03, -2.5494e-02, -1.2283e-03,  1.7044e-01,\n",
       "         -4.9746e-02, -1.8890e-02, -6.3689e-02, -3.3854e-02,  1.7237e-01,\n",
       "         -6.0978e-03,  2.1763e-02, -4.3416e-02,  7.5457e-02,  2.9024e-02,\n",
       "         -6.1067e-03,  5.2058e-03, -3.8862e-02, -6.2616e-03, -1.0429e-02,\n",
       "         -2.1213e-02, -3.2377e-02, -2.1631e-02,  1.1801e-02, -4.8583e-02,\n",
       "          7.9568e-03,  3.5445e-03, -9.7144e-03, -1.8767e-02, -7.1831e-03],\n",
       "        [-1.3719e-02,  2.5239e-03,  4.5559e-01,  6.0995e-03,  1.5561e-01,\n",
       "         -4.0269e-02, -1.2523e-02, -4.5647e-02,  6.8993e-03, -1.0656e-02,\n",
       "         -5.3215e-02, -1.6520e-02, -9.7398e-03,  2.4028e-03, -1.1964e-02,\n",
       "         -5.8181e-03, -1.9840e-02, -1.1012e-03, -1.8136e-02,  2.4025e-03,\n",
       "          2.2144e-04,  1.3957e-02, -2.2930e-02,  9.7942e-03, -1.6925e-02,\n",
       "         -5.8936e-03, -6.2075e-03,  1.3838e-02, -1.9537e-02,  2.1156e-05],\n",
       "        [-9.3610e-02,  2.0581e-03,  2.8441e-03, -1.7609e-02,  3.2889e-01,\n",
       "          2.9626e-03, -5.8919e-03, -9.6776e-03,  1.6282e-02, -3.5212e-02,\n",
       "         -1.0118e-01,  1.1903e-02,  2.6110e-02, -4.1736e-02,  1.5991e-02,\n",
       "         -3.8399e-03, -5.5625e-03, -3.4391e-02, -4.4615e-02, -2.1330e-02,\n",
       "          2.6017e-03,  2.2486e-04, -3.8361e-02,  4.8169e-02,  3.6406e-02,\n",
       "          1.2068e-02, -1.4367e-02, -1.3132e-02, -2.5259e-03, -1.0446e-02],\n",
       "        [-8.1073e-03,  4.2762e-02, -5.2288e-02,  4.5075e-03,  3.3414e-01,\n",
       "          2.7188e-03, -1.4417e-02, -1.8804e-02,  1.2070e-01,  4.3481e-03,\n",
       "          4.6848e-05, -4.1434e-02, -3.2482e-02, -1.1042e-02, -1.0281e-02,\n",
       "          1.1483e-03, -3.4628e-02,  2.5828e-03, -2.0375e-02,  2.2612e-02,\n",
       "         -4.7735e-03,  3.2990e-04, -2.8831e-02, -3.4438e-02,  1.6669e-04,\n",
       "         -4.7174e-02, -1.0905e-02, -8.9945e-03, -7.7301e-02, -7.6553e-03],\n",
       "        [-5.0751e-03, -4.3663e-02,  3.6039e-03,  2.9102e-01,  6.3041e-03,\n",
       "         -1.4183e-02, -6.8183e-02,  6.1243e-03, -4.5275e-02, -2.7905e-03,\n",
       "         -1.1467e-02, -7.7277e-02,  5.9998e-03,  1.1346e-02,  7.9721e-04,\n",
       "         -3.7884e-02, -1.1661e-02, -2.0265e-02,  1.7895e-02, -5.9277e-02,\n",
       "         -1.9032e-02, -7.0333e-03,  1.2288e-02,  3.3335e-02, -2.6896e-02,\n",
       "         -1.2939e-02, -1.0968e-01,  6.8002e-03, -1.8959e-02, -1.2946e-02],\n",
       "        [ 3.2406e-03,  5.2416e-01, -2.5509e-03, -2.0306e-03, -7.5386e-03,\n",
       "          1.6643e-01,  1.2463e-02,  4.3980e-03, -1.0011e-02,  1.3485e-02,\n",
       "         -2.3036e-03, -1.9625e-03, -1.6080e-02,  8.4582e-04,  9.0141e-03,\n",
       "         -1.5201e-02, -3.6115e-03,  5.4018e-03, -1.2612e-02, -3.4631e-03,\n",
       "         -3.1816e-02, -2.7081e-03, -3.3674e-02,  1.9163e-02,  5.9009e-03,\n",
       "         -1.0380e-02, -6.0363e-02,  3.3278e-03, -1.5466e-02,  4.0239e-04]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2906, 0.4918, 0.3760, 0.3835, 0.3477, 0.5571], device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_current_output_attribution.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4699, device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_output_attribution[0].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input_indices = torch.unique(input_indices)\n",
    "input_features[:, unique_input_indices].isnan().any()\n",
    "unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "# output_features.shape, unique_output_indices.shape\n",
    "# virtual_weights[unique_input_indices][:, unique_output_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6144, 6143]), tensor(6143, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_weights.shape, unique_output_indices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 795.99 GiB. GPU 0 has a total capacity of 15.73 GiB of which 14.37 GiB is free. Process 2007258 has 1.35 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 93.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m sparse_virtual_weights \u001b[38;5;241m=\u001b[39m virtual_weights[unique_input_indices][:, unique_output_indices]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Perform the sparse matrix multiplication\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m spar_attr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbi,ij->bij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_input_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_virtual_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# # Create a tensor to hold the full attribution\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# # Place the sparse attribution results in the correct positions in the full attribution tensor\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# full_attribution[:, unique_input_indices[:, None], unique_output_indices] = spar_attr\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 795.99 GiB. GPU 0 has a total capacity of 15.73 GiB of which 14.37 GiB is free. Process 2007258 has 1.35 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 93.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "\n",
    "def sparse_attribution(input_features, virtual_weights, input_indices, output_indices):\n",
    "    # Find unique input and output indices across the batch\n",
    "    unique_input_indices = torch.unique(input_indices)\n",
    "    unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "    # combine batch and sequence dimensions\n",
    "    input_features = rearrange(input_features, 'b s i -> (b s) i')\n",
    "\n",
    "    # Extract relevant slices of input_features and virtual_weights\n",
    "    sparse_input_features = input_features[:, unique_input_indices]\n",
    "    sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "\n",
    "    # Perform the sparse matrix multiplication\n",
    "    sparse_attribution = torch.einsum('bi,ij->bij', sparse_input_features, sparse_virtual_weights)\n",
    "\n",
    "    # Create a tensor to hold the full attribution\n",
    "    full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\n",
    "\n",
    "    # Place the sparse attribution results in the correct positions in the full attribution tensor\n",
    "    full_attribution[:, unique_input_indices[:, None], unique_output_indices] = sparse_attribution\n",
    "\n",
    "    return full_attribution\n",
    "\n",
    "# Usage\n",
    "# Assuming input_features, virtual_weights, input_indices, and output_indices are defined\n",
    "# attribution = sparse_attribution(input_features, virtual_weights, input_indices, output_indices)\n",
    "\n",
    "unique_input_indices = torch.unique(input_indices)\n",
    "unique_output_indices = torch.unique(output_indices)\n",
    "\n",
    "# Extract relevant slices of input_features and virtual_weights\n",
    "sparse_input_features = input_features[:, unique_input_indices]\n",
    "sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "\n",
    "# Perform the sparse matrix multiplication\n",
    "spar_attr = torch.einsum('bi,ij->bij', sparse_input_features, sparse_virtual_weights)\n",
    "\n",
    "# # Create a tensor to hold the full attribution\n",
    "# full_attribution = torch.zeros(input_features.shape[0], input_features.shape[1], virtual_weights.shape[1], device=input_features.device)\n",
    "\n",
    "# # Place the sparse attribution results in the correct positions in the full attribution tensor\n",
    "# full_attribution[:, unique_input_indices[:, None], unique_output_indices] = spar_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 6144]), torch.Size([4292]), torch.Size([6030]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, unique_input_indices.shape, unique_output_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 4292]), torch.Size([4292, 6030]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_input_features.shape, sparse_virtual_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features shape: torch.Size([64, 129, 6144])\n",
      "unique_input_indices shape: torch.Size([4292])\n",
      "Max value in unique_input_indices: 6142\n",
      "Min value in unique_input_indices: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"input_features shape:\", input_features.shape)\n",
    "unique_input_indices = torch.unique(input_indices)\n",
    "print(\"unique_input_indices shape:\", unique_input_indices.shape)\n",
    "print(\"Max value in unique_input_indices:\", unique_input_indices.max().item())\n",
    "print(\"Min value in unique_input_indices:\", unique_input_indices.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_input_indices = input_indices.unique()\n",
    "unique_output_indices = output_indices.unique()\n",
    "\n",
    "# # Extract relevant slices of input_features and virtual_weights\n",
    "sparse_input_features = input_features[:, unique_input_indices]\n",
    "sparse_virtual_weights = virtual_weights[unique_input_indices][:, unique_output_indices]\n",
    "# sparse_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4292,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = input_indices.unique().cpu().numpy()\n",
    "uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "input_features.index([0,1], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_attribution\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming input_features, virtual_weights, input_indices, and output_indices are defined\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# attribution = sparse_attribution(input_features, virtual_weights, input_indices, output_indices)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m unique_input_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m unique_output_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(output_indices)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Extract relevant slices of input_features and virtual_weights\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:997\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 997\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:911\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    903\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    905\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1160.81 GiB. GPU 0 has a total capacity of 15.73 GiB of which 13.95 GiB is free. Process 1969018 has 1.78 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 37.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, einsum\n\u001b[1;32m      2\u001b[0m input_features \u001b[38;5;241m=\u001b[39m rearrange(input_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s f -> (b s) f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb f1, f1 f2 -> b f1 f2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/einops/einops.py:907\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    905\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    906\u001b[0m pattern \u001b[38;5;241m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern, \u001b[38;5;241m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/test_env/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1160.81 GiB. GPU 0 has a total capacity of 15.73 GiB of which 13.95 GiB is free. Process 1969018 has 1.78 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 37.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, einsum\n",
    "input_features = rearrange(input_features, 'b s f -> (b s) f')\n",
    "attribution = einsum(input_features, virtual_weights, \"b f1, f1 f2 -> b f1 f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# RuntimeError: self must be a matrix\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# attribution = input_features * virtual_weights\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "attribution = torch.dot(input_features, virtual_weights)\n",
    "# RuntimeError: self must be a matrix\n",
    "# attribution = input_features * virtual_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8256, 6144]), torch.Size([6144, 6143]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, virtual_weights.shape\n",
    "attribution = torch.einsum('bi,ij->bij', input_features, virtual_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = rearrange(attribution, 'b s f -> (b s) f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 129, 6143])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indices = rearrange(output_indices, 'b s f -> (b s) f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_indices[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attribution = input_features @ virtual_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 129, 6144]), torch.Size([6144, 6143]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, virtual_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate attribution = act*gradient\n",
    "\n",
    "# I believe this is equivalent to the weights of the activations (ignore biases)\n",
    "# It'd be good to actually verify this is the case\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    tr_dec = transcoder.decoder.weight\n",
    "    #TODO: we removed the last weight to help w/ knowing .T and shape. \n",
    "    final_enc = sae_final.encoder.weight[:-1]\n",
    "    virtual_weights = tr_dec.T @ final_enc.T\n",
    "\n",
    "    act_res_mid = act_res_mid.to(device)\n",
    "    input_features, input_acts, input_indices = transcoder.encode(act_res_mid, return_topk=True)\n",
    "    mlp_out_hat = transcoder.decoder(input_features)\n",
    "\n",
    "    output_features, output_acts, output_indices = sae_final.encode(mlp_out_hat + act_res_mid, return_topk=True)\n",
    "\n",
    "    # For efficient gradient calculation, we can get the nonzero_indices of both input & output feature\n",
    "\n",
    "    # W_input = transcoder.decoder.weight[input_indices]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 6144]), torch.Size([64, 129, 30]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcoder.decoder.weight.shape, input_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(self, x: torch.Tensor, return_topk: bool = False):\n",
    "#     post_relu_feat_acts_BF = nn.functional.relu(self.encoder(x - self.b_dec))\n",
    "#     post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)\n",
    "\n",
    "#     # We can't split immediately due to nnsight\n",
    "#     tops_acts_BK = post_topk.values\n",
    "#     top_indices_BK = post_topk.indices\n",
    "\n",
    "#     buffer_BF = torch.zeros_like(post_relu_feat_acts_BF)\n",
    "#     encoded_acts_BF = buffer_BF.scatter_(dim=-1, index=top_indices_BK, src=tops_acts_BK)\n",
    "\n",
    "#     if return_topk:\n",
    "#         return encoded_acts_BF, tops_acts_BK, top_indices_BK\n",
    "#     else:\n",
    "#         return encoded_acts_BF\n",
    "\n",
    "# def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     return self.decoder(x) + self.b_dec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
